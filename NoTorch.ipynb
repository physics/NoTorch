{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-28T22:33:32.622146Z",
     "start_time": "2023-07-28T22:33:32.554643Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def sigmoid(z):\n",
    "  return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "  return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "class NoTorch:\n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "    self.n_layers = len(layers)\n",
    "    self.biases = [np.random.randn(nodes, 1) for nodes in layers[1:]]\n",
    "    self.weights = [np.random.randn(nodes_cur, nodes_last) / np.sqrt(nodes_last) for nodes_cur, nodes_last in zip(layers[1:], layers[:-1])]\n",
    "  def SGD(self, training_data, epochs, batch_sz, eta, lmbda, test_data):\n",
    "    training_data = list(training_data)\n",
    "    test_data = list(test_data)\n",
    "    n = len(training_data)\n",
    "    for j in range(epochs):\n",
    "      random.shuffle(training_data)\n",
    "      batches = [training_data[k : k + batch_sz] for k in range(0, n, batch_sz)]\n",
    "      for batch in batches:\n",
    "        self.update_parameters(batch, eta, lmbda, n)\n",
    "      print(\"Accuracy after epoch {}: {}%\".format(j + 1, self.accuracy(test_data)))\n",
    "  def update_parameters(self, batch, eta, lmbda, n):\n",
    "    nabla_b = [np.zeros(np.shape(b)) for b in self.biases]\n",
    "    nabla_w = [np.zeros(np.shape(w)) for w in self.weights]\n",
    "    m = len(batch)\n",
    "    for x, y in batch:\n",
    "      delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "      nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "      nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "    self.weights = [(1 - eta * lmbda / n) * w - (eta / m) * nw for w, nw in zip(self.weights, nabla_w)]\n",
    "    self.biases = [b - (eta / m) * nb for b, nb in zip(self.biases, nabla_b)]\n",
    "  def feedforward(self, a):\n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "      a = sigmoid(np.matmul(w, a) + b)\n",
    "    return a\n",
    "  def accuracy(self, data):\n",
    "    n = len(data)\n",
    "    correct = 0\n",
    "    for x, y in data:\n",
    "      prediction = np.argmax(self.feedforward(x))\n",
    "      correct += int(prediction == y)\n",
    "    return correct / n * 100\n",
    "  def backprop(self, x, y):\n",
    "    nabla_b = [None] * len(self.biases)\n",
    "    nabla_w = [None] * len(self.weights)\n",
    "    a, z = [x], [x]\n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "      z.append(np.matmul(w, a[-1]) + b)\n",
    "      a.append(sigmoid(z[-1]))\n",
    "    delta = a[-1] - y\n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.matmul(delta, a[-2].transpose())\n",
    "    for l in range(2, self.n_layers):\n",
    "      delta = sigmoid_prime(z[-l]) * np.matmul(self.weights[-l+1].transpose(), delta)\n",
    "      nabla_b[-l] = delta\n",
    "      nabla_w[-l] = np.matmul(delta, a[-l-1].transpose())\n",
    "    return nabla_b, nabla_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "TRAIN, VALIDATION, TEST = mnist_loader.load_data_wrapper()\n",
    "EPOCHS = 30\n",
    "ETA = 0.5\n",
    "LMBDA = 5\n",
    "BATCH_SZ = 10\n",
    "net = NoTorch([784, 100, 100, 10])\n",
    "net.SGD(TRAIN, EPOCHS, BATCH_SZ, ETA, LMBDA, TEST)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
